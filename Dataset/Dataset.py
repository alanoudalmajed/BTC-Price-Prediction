# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hxg7O9bF8gT62x28shiNrrXPBGZovnnf

# **1.	Data Collection**
"""

# Importing libraries
import requests  # For making HTTP requests
from bs4 import BeautifulSoup  # For web scraping
import pandas as pd  # For data manipulation and analysis
import numpy as np  # For numerical operations
import matplotlib.pyplot as plt  # For data visualization
import seaborn as sns  # For enhanced data visualization

"""### **1.1 Historical BTC prices:**
The dataset includes daily Bitcoin prices from May 2018 to September 2024 from Investing.com
"""

# Load dataset
BTC_USD_Historical_Data = pd.read_csv('BTC_USD Historical Data.csv')

# Rename columns in BTC data
BTC_USD_Historical_Data = BTC_USD_Historical_Data.rename(columns={
    'Price': 'BTC_Price',
    'Open': 'BTC_Open',
    'High': 'BTC_High',
    'Low': 'BTC_Low',
    'Vol.': 'BTC_Vol.',
    'Change': 'BTC_Change'
})

BTC_USD_Historical_Data

"""### **1.2	Economic Indicators:**
Data on various economic indicators, including  S&P 500, oil prices, gold prices, and other relevant economic variables, were collected from Investing.com, Argam.com, and EIA.com.
For brent_oil, we used API to extract data from a website. These indicators were integrated to clarify the potential economic factors influencing Bitcoin prices.
"""

# For brent_oil, we used API to extract data from a website

# API Data Fetch: Brent Oil
url = "https://api.eia.gov/v2/petroleum/pri/spt/data/"
params = {
    "api_key": "ID2PfupJmuavOIyurfbifgqnSUItjTtwWQCjyJcM",
    "frequency": "daily",
    "data[0]": "value",
    "facets[series][]": "RBRTE",
    "start": "2015-01-01",
    "end": "2024-09-30",
    "sort[0][column]": "period",
    "sort[0][direction]": "desc",
    "offset": 0,
    "length": 5000
}

# Fetch the data
response = requests.get(url, params=params)
if response.status_code == 200:
    data = response.json()
    records = data.get("response", {}).get("data", [])
    brent_oil = pd.DataFrame(records)[['period', 'value']].rename(
        columns={'period': 'Date', 'value': 'BrentOil_Price'}
    )
    brent_oil['Date'] = pd.to_datetime(brent_oil['Date'])
    brent_oil.to_excel("BrentOil.xlsx", index=False)
    print("Brent Oil data saved to 'BrentOil.xlsx'")
else:
    print(f"Failed to fetch Brent Oil data. Status code: {response.status_code}")

# Load Other Datasets
dowjones_data = pd.read_csv('Dow Jones .csv')
nasdaq_data = pd.read_csv('NASDAQ .csv')
gold_data = pd.read_excel('Gold.xlsx')
SP_data = pd.read_csv('S&P 500.csv')

# Convert Dates and Select Relevant Columns
datasets = [BTC_USD_Historical_Data, dowjones_data, nasdaq_data, gold_data, SP_data, brent_oil]
for df in datasets:
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

# Select relevant columns and rename for consistency
dowjones_data = dowjones_data[['Date', 'Price']].rename(columns={'Price': 'DowJones_Price'})
nasdaq_data = nasdaq_data[['Date', 'Price']].rename(columns={'Price': 'Nasdaq_Price'})
SP_data = SP_data[['Date', 'Price']].rename(columns={'Price': 'S&P_Price'})

"""### **Merge for Historical BTC prices & Economic Indicators**"""

# Merge Datasets
# Merge all datasets using an outer join to preserve all dates
HE = BTC_USD_Historical_Data
HE = pd.merge(HE, dowjones_data, on='Date', how='outer')
HE = pd.merge(HE, nasdaq_data, on='Date', how='outer')
HE = pd.merge(HE, SP_data, on='Date', how='outer')
HE = pd.merge(HE, gold_data, on='Date', how='outer')
HE = pd.merge(HE, brent_oil, on='Date', how='outer')

# Sort data by date for consistency
HE = HE.sort_values(by='Date').reset_index(drop=True)

# Final Output
HE

"""## **Pre-processing for Historical BTC prices & Economic Indicators**"""

# Step 1: Convert 'BTC_Vol.' to numeric using the existing function
def convert_volume(value):
    if isinstance(value, str):  # Only process strings
        if 'K' in value:
            return float(value.replace('K', '')) * 1_000  # Convert 'K' to thousands
        elif 'M' in value:
            return float(value.replace('M', '')) * 1_000_000  # Convert 'M' to millions
        elif 'B' in value:
            return float(value.replace('B', '')) * 1_000_000_000  # Convert 'B' to billions
        else:
            return float(value)  # Convert other strings to float
    return value  # Return the value as is if it's not a string

# Apply the function to the BTC_Vol. column
HE['BTC_Vol.'] = HE['BTC_Vol.'].apply(convert_volume)

# Step 2: Remove commas and strip whitespace from the columns, ensuring we work with strings first
columns_to_clean = ['BTC_Price', 'BTC_Open', 'BTC_High', 'BTC_Low', 'DowJones_Price', 'Nasdaq_Price', 'S&P_Price', 'BrentOil_Price']
for col in columns_to_clean:
    # Convert to string first (in case it has been converted to numeric) before applying string operations
    HE[col] = HE[col].astype(str).str.replace(',', '').str.strip()

# Step 3: Convert columns to numeric with error handling
numeric_columns = ['BTC_Price', 'BTC_Open', 'BTC_High', 'BTC_Low', 'BTC_Vol.',
                   'DowJones_Price', 'Nasdaq_Price', 'S&P_Price', 'Gold_Price', 'BrentOil_Price']

# Convert the columns to numeric, coercing errors into NaN
for col in numeric_columns:
    HE[col] = pd.to_numeric(HE[col], errors='coerce')

# Step 4: Clean 'Change %' column by converting to decimal
def clean_percentage(value):
    if isinstance(value, str) and '%' in value:
        return float(value.replace('%', '').strip()) / 100  # Convert to decimal
    return value

# Apply the percentage conversion function to 'Change %'
HE['Change %'] = HE['Change %'].apply(clean_percentage)

# Step 5: Reapply the conversion for all columns to ensure the 'Change %' is correctly converted
numeric_columns = ['BTC_Price', 'BTC_Open', 'BTC_High', 'BTC_Low', 'BTC_Vol.',
                   'DowJones_Price', 'Nasdaq_Price', 'S&P_Price', 'Gold_Price',
                   'BrentOil_Price', 'Change %']

# Convert all applicable columns to numeric types
for col in numeric_columns:
    HE[col] = pd.to_numeric(HE[col], errors='coerce')

# Step 6: Inspect the DataFrame after preprocessing
HE

# Calculate the correlation matrix
correlation_matrix = HE.corr()

# Plot the correlation heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix")
plt.show()

"""## **1.3	Sentiment Analysis**
Sentiment analysis was done by web scraping over 2,700 pages of news articles about Bitcoin, extracting about 93,870 news titles using a natural language processing (NLP) model like bidirectional encoder representations from transformers (BERT), which specializes in capturing contextual information from language, was employed to help the system understand the context of an ambiguous text.
"""

# Base URL of the cryptocurrency news page
base_url = 'https://www.investing.com/news/cryptocurrency-news'

# Headers to simulate a browser visit
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.5938.132 Safari/537.36'
}
# Lists to store the data
all_times = []
all_titles = []

crypto_news = pd.read_excel("crypto_news.xlsx")
crypto_news

from tqdm import tqdm
from transformers import pipeline

# Load the BERT sentiment analysis pipeline
classifier = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# Function to process a batch of texts and perform sentiment analysis
def process_batch(texts):
    results = classifier(texts, truncation=True, max_length=512)
    sentiments = []  # List to store sentiment labels
    scores = []  # List to store sentiment scores

    # Loop through results to classify and score each sentiment
    for res in results:
        label = res['label']
        score = res['score']
        # Categorize sentiment based on the star rating
        if label == "1 star" or label == "2 stars":
            sentiments.append("Negative")
            scores.append(-score)  # Negative sentiment score
        elif label == "4 stars" or label == "5 stars":
            sentiments.append("Positive")
            scores.append(score)  # Positive sentiment score
        else:
            sentiments.append("Neutral")
            scores.append(0)  # Neutral sentiment score

    return sentiments, scores

# Initialize a DataFrame to store results
news_analysis = crypto_news.copy()
news_analysis['Sentiment'] = None
news_analysis['Sentiment_Score'] = None

# Process the news titles in batches
batch_size = 32
for i in tqdm(range(0, len(crypto_news), batch_size), desc="Processing Batches"):
    # Extract a batch of titles
    batch_titles = crypto_news['Title'][i:i + batch_size].tolist()

    # Get sentiment results for the batch
    sentiments, scores = process_batch(batch_titles)

    # Store the sentiment labels and scores in the DataFrame
    news_analysis.loc[i:i + batch_size - 1, 'Sentiment'] = sentiments
    news_analysis.loc[i:i + batch_size - 1, 'Sentiment_Score'] = scores

# Display the resulting DataFrame with sentiments
print(news_analysis.head())

# Define the function to aggregate sentiments by date
def aggregate_sentiment_by_date(group):
    # Calculate the Weighted Average Sentiment (mean of sentiment scores)
    weighted_average = group['Sentiment_Score'].mean()

    # Find the Most Influential Sentiment (based on highest absolute score)
    most_influential_row = group.loc[group['Sentiment_Score'].abs().idxmax()]
    most_influential = most_influential_row['Sentiment']

    # Find the Majority Sentiment (most common sentiment)
    majority_sentiment = group['Sentiment'].value_counts().idxmax()

    # Return the aggregated results as a pandas Series
    return pd.Series({
        'Weighted_Average_Sentiment': weighted_average,
        'Most_Influential_Sentiment': most_influential,
        'Majority_Sentiment': majority_sentiment
    })
# Group the news_analysis DataFrame by Date and apply the aggregation function
Sentiment_analysis = news_analysis.groupby('Time').apply(aggregate_sentiment_by_date).reset_index()

Sentiment_analysis = pd.read_excel("Sentiment_analysis.xlsx")
Sentiment_analysis

"""# **Merge Datasets:**
**1- BTC prices & Economic Indicators**
**2- Sentiment Analysis**

# **Final result unifined data set included:**
 **BTC prices & Economic Indicators & Sentiment Analysis**
"""

# Merge the datasets on the 'Date' column with 'inner' join
Dataset = pd.merge(HE , Sentiment_analysis, on='Date', how='inner')

# Reorder columns
sentiment_columns = ['Weighted_Average_Sentiment', 'Most_Influential_Sentiment', 'Majority_Sentiment']
remaining_columns = [col for col in Dataset.columns if col not in sentiment_columns]

Dataset = Dataset[remaining_columns + sentiment_columns]

# Display the resulting DataFrame
Dataset